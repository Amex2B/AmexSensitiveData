{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Dataset({\n",
      "    features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 155.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 214.53 examples/s]\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define and load the tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lakshyakh93/deberta_finetuned_pii\")\n",
    "train_set = load_dataset(\"ai4privacy/pii-masking-300k\", split='train')\n",
    "dataset = train_set.shuffle().select(range(20))\n",
    "print(dataset)\n",
    "\n",
    "# Define label mapping\n",
    "label_map = {\"O\": 0, \"TIME\": 1, \"DATE\": 2, \"LASTNAME1\": 3, \"LASTNAME2\": 4, \"EMAIL\": 5, \"SOCIALNUMBER\": 6}\n",
    "\n",
    "# Function to tokenize and align labels\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example['source_text'], truncation=True, padding='max_length', max_length=512)\n",
    "    labels = [\"O\"] * len(tokenized_inputs[\"input_ids\"])\n",
    "    \n",
    "    # Adjust indexing to fit the tokenization\n",
    "    example['source_text'] = example['source_text'][:512]  # Ensure source_text matches max_length\n",
    "\n",
    "    for span in example[\"privacy_mask\"]:\n",
    "        start, end, label_name = span[\"start\"], span[\"end\"], span[\"label\"]\n",
    "        label_id = label_map.get(label_name, 0)\n",
    "        \n",
    "        span_tokens = tokenizer(example[\"source_text\"][start:end], add_special_tokens=False).tokens()\n",
    "        # Properly create and align labels (assuming BERT tokenizer)\n",
    "        for i, token in enumerate(tokenized_inputs.tokens()):\n",
    "            if token in span_tokens:\n",
    "                labels[i] = label_id\n",
    "\n",
    "    numeric_labels = [label_map.get(label, -100) for label in labels]\n",
    "    numeric_labels += [-100] * (len(tokenized_inputs[\"input_ids\"]) - len(numeric_labels))\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = numeric_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Apply function to dataset\n",
    "encoded_dataset = dataset.map(tokenize_and_align_labels, batched=False)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "input_ids = torch.tensor(encoded_dataset[\"input_ids\"])\n",
    "attention_mask = torch.tensor(encoded_dataset[\"attention_mask\"])\n",
    "labels = torch.tensor(encoded_dataset[\"labels\"])\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"lakshyakh93/deberta_finetuned_pii\")\n",
    "\n",
    "# Define metric computation\n",
    "metric = evaluate.load(\"accuracy\")  # You may want to load relevant metrics for token classification\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=2)  # Note: axis=2 for token classification\n",
    "    true_labels = labels != -100  # Masking out unnecessary labels\n",
    "    \n",
    "    # Flatten predictions and true_labels to compute accuracy\n",
    "    flattened_predictions = predictions[true_labels]\n",
    "    flattened_labels = labels[true_labels]\n",
    "    \n",
    "    results = metric.compute(references=flattened_labels, predictions=flattened_predictions)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Tokenize validation set\n",
    "val_set = load_dataset(\"ai4privacy/pii-masking-300k\", split='validation')  # Ensure you have a validation set\n",
    "small_val = val_set.shuffle().select(range(20))\n",
    "encoded_small_val = small_val.map(tokenize_and_align_labels, batched=False)\n",
    "\n",
    "# Training arguments\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    gradient_accumulation_steps=2,  # Adjust if needed\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_small_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [02:45<04:18, 25.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4130392074584961, 'eval_accuracy': 0.9286088139851355, 'eval_runtime': 35.6973, 'eval_samples_per_second': 0.56, 'eval_steps_per_second': 0.084, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [04:52<02:22, 28.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0259, 'grad_norm': 0.8904018998146057, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [05:33<02:22, 28.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04530291631817818, 'eval_accuracy': 0.987438500994452, 'eval_runtime': 37.0441, 'eval_samples_per_second': 0.54, 'eval_steps_per_second': 0.081, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [10:01<00:00, 40.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01584729738533497, 'eval_accuracy': 0.9995812833664818, 'eval_runtime': 62.3534, 'eval_samples_per_second': 0.321, 'eval_steps_per_second': 0.048, 'epoch': 3.0}\n",
      "{'train_runtime': 600.9944, 'train_samples_per_second': 0.1, 'train_steps_per_second': 0.025, 'train_loss': 1.3604229042927425, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=1.3604229042927425, metrics={'train_runtime': 600.9944, 'train_samples_per_second': 0.1, 'train_steps_per_second': 0.025, 'total_flos': 18303156633600.0, 'train_loss': 1.3604229042927425, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:37<00:00, 12.42s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.01584729738533497,\n",
       " 'eval_accuracy': 0.9995812833664818,\n",
       " 'eval_runtime': 62.1961,\n",
       " 'eval_samples_per_second': 0.322,\n",
       " 'eval_steps_per_second': 0.048,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PREFIX][PREFIX][PREFIX][PREFIX][PREFIX][FIRSTNAME][FIRSTNAME][FIRSTNAME][PREFIX][PREFIX][PREFIX][PREFIX][PREFIX][PREFIX][PHONE_NUMBER][PREFIX][PREFIX][PREFIX][PREFIX][PREFIX][PREFIX],[PREFIX][PREFIX][PREFIX][PREFIX][PREFIX][PREFIX][PREFIX][PREFIX][PREFIX][PREFIX][PREFIX][USERNAME][EMAIL][USERNAME]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize a token classification pipeline\n",
    "pii_pipeline = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"\")\n",
    "\n",
    "# Function to apply PII masking to a specific text column\n",
    "def redact_column(dataset, column_name):\n",
    "    redacted_texts = []\n",
    "\n",
    "    for text in dataset[column_name]:\n",
    "        # Get model predictions for the text\n",
    "        predictions = pii_pipeline(text)\n",
    "        \n",
    "        # Replace PII spans with placeholders\n",
    "        redacted_text = text\n",
    "        for entity in sorted(predictions, key=lambda x: x['start'], reverse=True):  # Sort in reverse to avoid shifting positions\n",
    "            label = entity['entity_group']\n",
    "            redacted_text = redacted_text[:entity['start']] + f\"[{label}]\" + redacted_text[entity['end']:]\n",
    "        \n",
    "        redacted_texts.append(redacted_text)\n",
    "    \n",
    "    return redacted_texts\n",
    "\n",
    "df = pd.DataFrame({'source_text': ['hi my name is shubhangi, and my phone number is 239-123-1238 and I live in Los Angeles, California. My email is shubhangiwaldiya@gmail.com']})\n",
    "\n",
    "# Display the redacted text\n",
    "print(redact_column(df, 'source_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['source_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported the dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "train_set1=load_dataset('ai4privacy/pii-masking-300k', split='train')\n",
    "val_set=load_dataset('ai4privacy/pii-masking-300k', split='validation')\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), '..', 'data', 'maker_day_shrieyaa_stella_mini200_df.csv')\n",
    "train_df2 = pd.read_csv(filepath)\n",
    "train_set2 = Dataset.from_pandas(train_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "(200, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(47728, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "print(type(train_set2))\n",
    "print(train_set2.shape)\n",
    "val_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'unmasked_text', 'masked_text'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Select 100 random indices\n",
    "# random_indices = random.sample(range(len(train_set1)), 10)\n",
    "random_indices = random.sample(range(len(train_set2)), 10)\n",
    "\n",
    "# Get the random samples\n",
    "# random_samples = train_set1.select(random_indices)\n",
    "random_samples = train_set2.select(random_indices)\n",
    "\n",
    "print(random_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'unmasked_text', 'masked_text'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Select 100 random indices\n",
    "# random_indices = random.sample(range(len(train_set1)), 10)\n",
    "random_indices = random.sample(range(len(train_set2)), 10)\n",
    "\n",
    "# Get the random samples\n",
    "random_samples = train_set2.select(random_indices)\n",
    "\n",
    "print(random_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inqu', 32223),\n",
       " ('Ä Darkness', 39824),\n",
       " ('Ä philosophies', 40728),\n",
       " ('MB', 8651),\n",
       " ('Ä Hugo', 18148),\n",
       " ('ibling', 46514),\n",
       " ('Ä Sonia', 23961),\n",
       " ('Ä scent', 26431),\n",
       " ('Ä crim', 29322),\n",
       " ('Ä later', 423)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('lakshyakh93/deberta_finetuned_pii')\n",
    "list(tokenizer.vocab.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\JoyChang\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# Define and load the tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('lakshyakh93/deberta_finetuned_pii')\n",
    "train_set1_selected = train_set1.select(range(5))\n",
    "\n",
    "# Define label mapping\n",
    "# label_map = {'O': 0, 'TIME': 1, 'DATE': 2, 'LASTNAME1': 3, 'LASTNAME2': 4, 'EMAIL': 5, 'SOCIALNUMBER': 6}\n",
    "label_map = {'O': 0, 'FIRSTNAME_1': 1, 'LASTNAME_1': 2, 'EMAIL_1': 3, 'PHONENUMBER_1': 4, 'SSN_1': 5, 'JOBTITLE_1': 6,\n",
    "             'SEX_1': 7, 'BUILDINGNUMBER_1': 8, 'STREET_1': 9, 'DOB_1': 10, 'USERNAME_1': 11, 'AGE_1': 12,\n",
    "             'PREFIX_1': 13, 'ACCOUNTNUMBER_1': 14}\n",
    "\n",
    "\n",
    "# Function to tokenize and align labels\n",
    "def tokenize_and_align_labels_train(example):\n",
    "    # tokenized_inputs = tokenizer(example['source_text'], truncation=True, padding='max_length', max_length=512)\n",
    "    tokenized_inputs = tokenizer(example['unmasked_text'], truncation=True, padding='max_length', max_length=512)\n",
    "    labels = ['O'] * len(tokenized_inputs['input_ids'])\n",
    "    \n",
    "    # Adjust indexing to fit the tokenization\n",
    "    # example['source_text'] = example['source_text'][:512]  # Ensure source_text matches max_length\n",
    "    example['unmasked_text'] = example['unmasked_text'][:512]  # Ensure source_text matches max_length\n",
    "\n",
    "    # Extract spans from masked_text\n",
    "    masked_text = example['masked_text']\n",
    "    # Find all labels in masked_text using regex\n",
    "    matches = re.finditer(r'\\[([A-Z_]+)\\]', masked_text)\n",
    "\n",
    "    # Create spans and assign labels\n",
    "    for match in matches:\n",
    "        label_name = match.group(1)  # Get the label from the match\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        label_id = label_map.get(label_name, 0)\n",
    "\n",
    "    # for span in example['privacy_mask']:\n",
    "    #     start, end, label_name = span['start'], span['end'], span['label']\n",
    "    #     label_id = label_map.get(label_name, 0)\n",
    "        \n",
    "        # span_tokens = tokenizer(example['source_text'][start:end], add_special_tokens=False).tokens()\n",
    "        span_tokens = tokenizer(example['unmasked_text'][start:end], add_special_tokens=False).tokens()\n",
    "\n",
    "        # Properly create and align labels (assuming BERT tokenizer)\n",
    "        for i, token in enumerate(tokenized_inputs.tokens()):\n",
    "            if token in span_tokens:\n",
    "                labels[i] = label_id\n",
    "\n",
    "    numeric_labels = [label_map.get(label, -100) for label in labels]\n",
    "    numeric_labels += [-100] * (len(tokenized_inputs['input_ids']) - len(numeric_labels))\n",
    "    \n",
    "    tokenized_inputs['labels'] = numeric_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and align labels\n",
    "def tokenize_and_align_labels_val(example):\n",
    "    tokenized_inputs = tokenizer(example['source_text'], truncation=True, padding='max_length', max_length=512)\n",
    "    labels = ['O'] * len(tokenized_inputs['input_ids'])\n",
    "    \n",
    "    # Adjust indexing to fit the tokenization\n",
    "    example['source_text'] = example['source_text'][:512]  # Ensure source_text matches max_length\n",
    "\n",
    "    # Create spans and assign labels\n",
    "    for span in example['privacy_mask']:\n",
    "        start, end, label_name = span['start'], span['end'], span['label']\n",
    "        label_id = label_map.get(label_name, 0)\n",
    "        \n",
    "        span_tokens = tokenizer(example['source_text'][start:end], add_special_tokens=False).tokens()\n",
    "\n",
    "        # Properly create and align labels (assuming BERT tokenizer)\n",
    "        for i, token in enumerate(tokenized_inputs.tokens()):\n",
    "            if token in span_tokens:\n",
    "                labels[i] = label_id\n",
    "\n",
    "    numeric_labels = [label_map.get(label, -100) for label in labels]\n",
    "    numeric_labels += [-100] * (len(tokenized_inputs['input_ids']) - len(numeric_labels))\n",
    "    \n",
    "    tokenized_inputs['labels'] = numeric_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3a5f0c668542ae842154eb479f72e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JoyChang\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply function to dataset\n",
    "# encoded_dataset = train_set1_selected.map(tokenize_and_align_labels, batched=False)\n",
    "encoded_dataset = train_set2.map(tokenize_and_align_labels_train, batched=False)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "input_ids = torch.tensor(encoded_dataset['input_ids'])\n",
    "attention_mask = torch.tensor(encoded_dataset['attention_mask'])\n",
    "labels = torch.tensor(encoded_dataset['labels'])\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained('lakshyakh93/deberta_finetuned_pii')\n",
    "\n",
    "# Define metric computation\n",
    "metric = evaluate.load('accuracy')  # You may want to load relevant metrics for token classification\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=2)  # Note: axis=2 for token classification\n",
    "    true_labels = labels != -100  # Masking out unnecessary labels\n",
    "    \n",
    "    # Flatten predictions and true_labels to compute accuracy\n",
    "    flattened_predictions = predictions[true_labels]\n",
    "    flattened_labels = labels[true_labels]\n",
    "    \n",
    "    results = metric.compute(references=flattened_labels, predictions=flattened_predictions)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Tokenize validation set\n",
    "small_val = val_set.shuffle(seed=123).select(range(5))\n",
    "encoded_small_val = small_val.map(tokenize_and_align_labels_val, batched=False)\n",
    "\n",
    "# Training arguments\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    gradient_accumulation_steps=2,  # Adjust if needed\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_small_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606a9305144b4ca2bfda84b573d2055c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0466, 'grad_norm': 0.0, 'learning_rate': 0.036111111111111115, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a321d809e9c4b01913ce61fc0815faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_accuracy': 1.0, 'eval_runtime': 4.0968, 'eval_samples_per_second': 1.22, 'eval_steps_per_second': 0.244, 'epoch': 0.96}\n",
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.022222222222222223, 'epoch': 1.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ef15f1a3c84869953431ac7ed0dffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_accuracy': 1.0, 'eval_runtime': 3.9273, 'eval_samples_per_second': 1.273, 'eval_steps_per_second': 0.255, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.008333333333333333, 'epoch': 2.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a8d2bd78e643fa9c61710030ee9f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_accuracy': 1.0, 'eval_runtime': 3.5882, 'eval_samples_per_second': 1.393, 'eval_steps_per_second': 0.279, 'epoch': 2.88}\n",
      "{'train_runtime': 2545.3469, 'train_samples_per_second': 0.236, 'train_steps_per_second': 0.014, 'train_loss': 0.5685017903645834, 'epoch': 2.88}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=0.5685017903645834, metrics={'train_runtime': 2545.3469, 'train_samples_per_second': 0.236, 'train_steps_per_second': 0.014, 'total_flos': 175710303682560.0, 'train_loss': 0.5685017903645834, 'epoch': 2.88})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19e496786204aba8d8c330ff36cfe0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0,\n",
       " 'eval_accuracy': 1.0,\n",
       " 'eval_runtime': 3.4,\n",
       " 'eval_samples_per_second': 1.471,\n",
       " 'eval_steps_per_second': 0.294,\n",
       " 'epoch': 2.88}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

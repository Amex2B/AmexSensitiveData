{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "Fmn9974mQ_be",
        "outputId": "20065897-2785-4670-e100-fb285992d5c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chardet"
      ],
      "metadata": {
        "id": "NOGaH53HiueD",
        "outputId": "bc9d1a7a-98a3-4da7-f1df-be270344a297",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "lgd9lg-0OqVX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and pre-trained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lakshyakh93/deberta_finetuned_pii\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"lakshyakh93/deberta_finetuned_pii\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "9Zd8IjNWjCLJ",
        "outputId": "25556869-9d33-4c05-cb1c-d2d3ba2a7460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the mini dataset (200 row)\n",
        "# import chardet\n",
        "\n",
        "# filepath = os.path.join(os.getcwd(), 'mini_df_200_row_biolabels.csv')\n",
        "\n",
        "# # Detect encoding\n",
        "# with open(filepath, 'rb') as f:\n",
        "#     result = chardet.detect(f.read())\n",
        "#     print(\"Detected file encoding:\", result['encoding'])\n",
        "\n",
        "# # Read the file using the detected encoding\n",
        "# mini_df = pd.read_csv(filepath, encoding=result['encoding'])\n",
        "\n",
        "# # Proportional split ratios\n",
        "# train_ratio = 0.7\n",
        "# val_ratio = 0.15\n",
        "# test_ratio = 0.15\n",
        "\n",
        "# # Since you have 200 rows, no need for the 3,000 cap, as this won't be needed\n",
        "# train_size = int(len(mini_df) * train_ratio)\n",
        "# val_size = int(len(mini_df) * val_ratio)\n",
        "# test_size = len(mini_df) - train_size - val_size  # Remaining rows for the test set\n",
        "\n",
        "# # Split the dataset into train, validation, and test sets\n",
        "# train_df, temp_df = train_test_split(mini_df, test_size=1 - train_ratio, random_state=42)\n",
        "# val_df, test_df = train_test_split(temp_df, test_size=test_ratio / (test_ratio + val_ratio), random_state=42)\n",
        "\n",
        "# # Print dataset sizes to confirm\n",
        "# print(f\"Training size: {len(train_df)}\")\n",
        "# print(f\"Validation size: {len(val_df)}\")\n",
        "# print(f\"Test size: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "R9NmJEwO-LUr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # For the cleaned mini dataframe\n",
        "# # Create a mapping for the bio_labels\n",
        "# data_label_list = []\n",
        "\n",
        "# # Extract unique labels from the 'bio_labels' column\n",
        "# def get_unique_labels(df):\n",
        "#     unique_labels = set()\n",
        "#     for labels in df['bio_labels']:  # Iterate through the 'bio_labels' column\n",
        "#         # Since bio_labels is a string representation of a list, we need to convert it back to a list\n",
        "#         labels = eval(labels)  # Convert the string to a list\n",
        "#         unique_labels.update(labels)  # Add all labels in the current row to the set\n",
        "#     return sorted(unique_labels)  # Return sorted labels for consistency\n",
        "\n",
        "# # Apply the function to the entire DataFrame\n",
        "# unique_labels = get_unique_labels(mini_df)  # Run on the whole DataFrame\n",
        "\n",
        "# # Print the unique labels\n",
        "# print(\"Data's Unique BIO Labels:\", unique_labels)\n",
        "\n",
        "# # Combine with the existing label_list (if applicable)\n",
        "# data_label_list = sorted(set(data_label_list + unique_labels))  # Ensure no duplicates\n",
        "# print(\"Data's Final Label List:\", data_label_list)\n",
        "# print(len(data_label_list))  # Number of unique labels\n",
        "\n",
        "# data_label_to_id = {label: idx for idx, label in enumerate(data_label_list)}\n",
        "# data_id_to_label = {idx: label for label, idx in data_label_to_id.items()}"
      ],
      "metadata": {
        "id": "BIOvLTGTFKfZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the proportional sizes\n",
        "total_samples = 2000\n",
        "train_ratio = 7\n",
        "val_ratio = 1.5\n",
        "test_ratio = 1.5\n",
        "\n",
        "total_ratio = train_ratio + val_ratio + test_ratio\n",
        "\n",
        "train_size = int(total_samples * (train_ratio / total_ratio))\n",
        "val_size = int(total_samples * (val_ratio / total_ratio))\n",
        "test_size = total_samples - train_size - val_size\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"Isotonic/pii-masking-200k\")\n",
        "\n",
        "# First, split off the test and validation sets\n",
        "first_split = dataset[\"train\"].train_test_split(test_size=(val_size + test_size)/len(dataset[\"train\"]), seed=42)\n",
        "\n",
        "# Then split the remaining data into train\n",
        "train_split = first_split[\"train\"].shuffle(seed=42).select(range(train_size))\n",
        "val_test_split = first_split[\"test\"].shuffle(seed=42).select(range(val_size + test_size))\n",
        "val_split = val_test_split.select(range(val_size))\n",
        "test_split = val_test_split.select(range(val_size, val_size + test_size))\n",
        "\n",
        "# Create the final DatasetDict\n",
        "dataset_splits = DatasetDict({\n",
        "    \"train\": train_split,\n",
        "    \"validation\": val_split,\n",
        "    \"test\": test_split\n",
        "})\n",
        "\n",
        "print(f\"Training size: {len(dataset_splits['train'])}\")\n",
        "print(f\"Validation size: {len(dataset_splits['validation'])}\")\n",
        "print(f\"Test size: {len(dataset_splits['test'])}\")"
      ],
      "metadata": {
        "id": "iHwCXYSAT6Ct",
        "outputId": "7dd2f4c1-5025-4fca-95a1-410ed227fcd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training size: 1400\n",
            "Validation size: 300\n",
            "Test size: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert test_dataset into a dataframe\n",
        "pii200k_train = dataset_splits['train'].to_pandas()\n",
        "pii200k_test = dataset_splits['test'].to_pandas()\n",
        "eng_train = pii200k_train[pii200k_train['language'] == 'en']\n",
        "eng_test = pii200k_test[pii200k_test['language'] == 'en']\n",
        "print(len(eng_train))\n",
        "print(len(eng_test))"
      ],
      "metadata": {
        "id": "6oL0nmfvVmTb",
        "outputId": "07fd6a87-747e-46d8-aba9-daa0c8df28c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "267\n",
            "68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rows where 'bio_labels' is not null (though unlikely given your data)\n",
        "valid_eng_train = eng_train[eng_train['bio_labels'].notnull()]\n",
        "valid_eng_test = eng_test[eng_test['bio_labels'].notnull()]\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def clean_bio_labels(bio_labels):\n",
        "    # Flatten the numpy array of bio_labels and remove any non-printable characters\n",
        "    bio_labels = [label for label in bio_labels.flatten() if isinstance(label, str)]\n",
        "    cleaned_labels = [re.sub(r'[^\\x20-\\x7E]', '', label) for label in bio_labels]\n",
        "    return cleaned_labels\n",
        "\n",
        "# Apply the cleaning function to each bio_labels entry\n",
        "eng_train['bio_labels'] = eng_train['bio_labels'].apply(clean_bio_labels)\n",
        "eng_test['bio_labels'] = eng_test['bio_labels'].apply(clean_bio_labels)\n",
        "\n",
        "# Check the resulting DataFrame\n",
        "print(f\"Number of valid rows in eng_train: {len(eng_train)}\")\n",
        "print(f\"Number of valid rows in eng_test: {len(eng_test)}\")"
      ],
      "metadata": {
        "id": "ZEH7NgmqZZby",
        "outputId": "b48cd7df-e172-417b-eb26-b1a7dd21b5f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of valid rows in eng_train: 267\n",
            "Number of valid rows in eng_test: 68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-24ddafc033f7>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  eng_train['bio_labels'] = eng_train['bio_labels'].apply(clean_bio_labels)\n",
            "<ipython-input-9-24ddafc033f7>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  eng_test['bio_labels'] = eng_test['bio_labels'].apply(clean_bio_labels)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_label_list = []\n",
        "\n",
        "# Extract unique BIO labels\n",
        "unique_labels = set()\n",
        "\n",
        "# Iterate through the 'bio_labels' column\n",
        "for labels in eng_train['bio_labels']:\n",
        "    unique_labels.update(labels)  # Add all unique labels from each array\n",
        "\n",
        "# Convert to sorted list for consistency\n",
        "unique_labels = sorted(unique_labels)\n",
        "\n",
        "print(\"Data's Unique BIO Labels:\", unique_labels)\n",
        "print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "\n",
        "# Combine with the existing label_list (if applicable)\n",
        "data_label_list = sorted(set(data_label_list + unique_labels))  # Ensure no duplicates\n",
        "print(\"Data's Final Label List:\", data_label_list)\n",
        "print(len(data_label_list))  # Number of unique labels\n",
        "\n",
        "data_label_to_id = {label: idx for idx, label in enumerate(data_label_list)}\n",
        "data_id_to_label = {idx: label for label, idx in data_label_to_id.items()}"
      ],
      "metadata": {
        "id": "NhsJl7B_jori",
        "outputId": "c66e5b7f-7e70-46bc-ad0f-1c9bbec94f72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data's Unique BIO Labels: ['B-ACCOUNTNAME', 'B-ACCOUNTNUMBER', 'B-AGE', 'B-AMOUNT', 'B-BIC', 'B-BITCOINADDRESS', 'B-BUILDINGNUMBER', 'B-CITY', 'B-COMPANYNAME', 'B-COUNTY', 'B-CREDITCARDCVV', 'B-CREDITCARDISSUER', 'B-CREDITCARDNUMBER', 'B-CURRENCY', 'B-CURRENCYCODE', 'B-CURRENCYNAME', 'B-CURRENCYSYMBOL', 'B-DATE', 'B-DOB', 'B-EMAIL', 'B-ETHEREUMADDRESS', 'B-EYECOLOR', 'B-FIRSTNAME', 'B-GENDER', 'B-HEIGHT', 'B-IBAN', 'B-IP', 'B-IPV4', 'B-IPV6', 'B-JOBAREA', 'B-JOBTITLE', 'B-JOBTYPE', 'B-LASTNAME', 'B-LITECOINADDRESS', 'B-MAC', 'B-MASKEDNUMBER', 'B-MIDDLENAME', 'B-NEARBYGPSCOORDINATE', 'B-ORDINALDIRECTION', 'B-PASSWORD', 'B-PHONEIMEI', 'B-PHONENUMBER', 'B-PIN', 'B-PREFIX', 'B-SECONDARYADDRESS', 'B-SEX', 'B-SSN', 'B-STATE', 'B-STREET', 'B-TIME', 'B-URL', 'B-USERAGENT', 'B-USERNAME', 'B-VEHICLEVIN', 'B-VEHICLEVRM', 'B-ZIPCODE', 'I-ACCOUNTNAME', 'I-ACCOUNTNUMBER', 'I-AGE', 'I-AMOUNT', 'I-BIC', 'I-BITCOINADDRESS', 'I-BUILDINGNUMBER', 'I-CITY', 'I-COMPANYNAME', 'I-COUNTY', 'I-CREDITCARDISSUER', 'I-CREDITCARDNUMBER', 'I-CURRENCY', 'I-CURRENCYCODE', 'I-CURRENCYNAME', 'I-CURRENCYSYMBOL', 'I-DATE', 'I-DOB', 'I-EMAIL', 'I-ETHEREUMADDRESS', 'I-EYECOLOR', 'I-FIRSTNAME', 'I-GENDER', 'I-HEIGHT', 'I-IBAN', 'I-IP', 'I-IPV4', 'I-IPV6', 'I-JOBAREA', 'I-JOBTITLE', 'I-JOBTYPE', 'I-LASTNAME', 'I-LITECOINADDRESS', 'I-MAC', 'I-MASKEDNUMBER', 'I-MIDDLENAME', 'I-NEARBYGPSCOORDINATE', 'I-PASSWORD', 'I-PHONEIMEI', 'I-PHONENUMBER', 'I-PIN', 'I-PREFIX', 'I-SECONDARYADDRESS', 'I-SSN', 'I-STATE', 'I-STREET', 'I-TIME', 'I-URL', 'I-USERAGENT', 'I-USERNAME', 'I-VEHICLEVIN', 'I-VEHICLEVRM', 'I-ZIPCODE', 'O']\n",
            "Number of unique labels: 110\n",
            "Data's Final Label List: ['B-ACCOUNTNAME', 'B-ACCOUNTNUMBER', 'B-AGE', 'B-AMOUNT', 'B-BIC', 'B-BITCOINADDRESS', 'B-BUILDINGNUMBER', 'B-CITY', 'B-COMPANYNAME', 'B-COUNTY', 'B-CREDITCARDCVV', 'B-CREDITCARDISSUER', 'B-CREDITCARDNUMBER', 'B-CURRENCY', 'B-CURRENCYCODE', 'B-CURRENCYNAME', 'B-CURRENCYSYMBOL', 'B-DATE', 'B-DOB', 'B-EMAIL', 'B-ETHEREUMADDRESS', 'B-EYECOLOR', 'B-FIRSTNAME', 'B-GENDER', 'B-HEIGHT', 'B-IBAN', 'B-IP', 'B-IPV4', 'B-IPV6', 'B-JOBAREA', 'B-JOBTITLE', 'B-JOBTYPE', 'B-LASTNAME', 'B-LITECOINADDRESS', 'B-MAC', 'B-MASKEDNUMBER', 'B-MIDDLENAME', 'B-NEARBYGPSCOORDINATE', 'B-ORDINALDIRECTION', 'B-PASSWORD', 'B-PHONEIMEI', 'B-PHONENUMBER', 'B-PIN', 'B-PREFIX', 'B-SECONDARYADDRESS', 'B-SEX', 'B-SSN', 'B-STATE', 'B-STREET', 'B-TIME', 'B-URL', 'B-USERAGENT', 'B-USERNAME', 'B-VEHICLEVIN', 'B-VEHICLEVRM', 'B-ZIPCODE', 'I-ACCOUNTNAME', 'I-ACCOUNTNUMBER', 'I-AGE', 'I-AMOUNT', 'I-BIC', 'I-BITCOINADDRESS', 'I-BUILDINGNUMBER', 'I-CITY', 'I-COMPANYNAME', 'I-COUNTY', 'I-CREDITCARDISSUER', 'I-CREDITCARDNUMBER', 'I-CURRENCY', 'I-CURRENCYCODE', 'I-CURRENCYNAME', 'I-CURRENCYSYMBOL', 'I-DATE', 'I-DOB', 'I-EMAIL', 'I-ETHEREUMADDRESS', 'I-EYECOLOR', 'I-FIRSTNAME', 'I-GENDER', 'I-HEIGHT', 'I-IBAN', 'I-IP', 'I-IPV4', 'I-IPV6', 'I-JOBAREA', 'I-JOBTITLE', 'I-JOBTYPE', 'I-LASTNAME', 'I-LITECOINADDRESS', 'I-MAC', 'I-MASKEDNUMBER', 'I-MIDDLENAME', 'I-NEARBYGPSCOORDINATE', 'I-PASSWORD', 'I-PHONEIMEI', 'I-PHONENUMBER', 'I-PIN', 'I-PREFIX', 'I-SECONDARYADDRESS', 'I-SSN', 'I-STATE', 'I-STREET', 'I-TIME', 'I-URL', 'I-USERAGENT', 'I-USERNAME', 'I-VEHICLEVIN', 'I-VEHICLEVRM', 'I-ZIPCODE', 'O']\n",
            "110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_label_list = model.config.id2label.values()  # Ensure this matches the fine-tuned model\n",
        "model_id_to_label = {int(k): v for k, v in model.config.id2label.items()}\n",
        "model_label_to_id = {v: int(k) for k, v in model_id_to_label.items()}\n",
        "\n",
        "print(\"Model's Label to ID:\", model_label_to_id)\n",
        "print(\"Model's ID to Label:\", model_id_to_label)\n",
        "print(len(model_label_list))"
      ],
      "metadata": {
        "id": "EejcPQbmwrK-",
        "outputId": "43dd4a75-1119-48fc-e5c4-0db820280e67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's Label to ID: {'B-PREFIX': 0, 'I-PREFIX': 1, 'B-FIRSTNAME': 2, 'I-FIRSTNAME': 3, 'B-MIDDLENAME': 4, 'B-LASTNAME': 5, 'I-LASTNAME': 6, 'O': 7, 'B-JOBDESCRIPTOR': 8, 'B-JOBTITLE': 9, 'I-JOBTITLE': 10, 'B-COMPANY_NAME': 11, 'I-COMPANY_NAME': 12, 'B-JOBAREA': 13, 'B-EMAIL': 14, 'I-EMAIL': 15, 'B-TIME': 16, 'I-TIME': 17, 'B-DATE': 18, 'I-DATE': 19, 'B-URL': 20, 'I-URL': 21, 'B-BITCOINADDRESS': 22, 'I-BITCOINADDRESS': 23, 'B-ETHEREUMADDRESS': 24, 'I-ETHEREUMADDRESS': 25, 'B-ACCOUNTNAME': 26, 'I-ACCOUNTNAME': 27, 'B-IBAN': 28, 'I-IBAN': 29, 'B-ACCOUNTNUMBER': 30, 'I-ACCOUNTNUMBER': 31, 'B-BIC': 32, 'I-BIC': 33, 'B-IPV4': 34, 'I-IPV4': 35, 'B-STREETADDRESS': 36, 'I-STREETADDRESS': 37, 'B-CITY': 38, 'I-CITY': 39, 'B-ZIPCODE': 40, 'I-ZIPCODE': 41, 'B-USERNAME': 42, 'I-USERNAME': 43, 'B-IPV6': 44, 'I-IPV6': 45, 'B-CREDITCARDNUMBER': 46, 'I-CREDITCARDNUMBER': 47, 'B-VEHICLEVIN': 48, 'I-VEHICLEVIN': 49, 'B-SUFFIX': 50, 'I-SUFFIX': 51, 'B-AMOUNT': 52, 'I-AMOUNT': 53, 'B-CURRENCY': 54, 'I-CURRENCY': 55, 'B-PASSWORD': 56, 'I-PASSWORD': 57, 'B-JOBTYPE': 58, 'B-STATE': 59, 'B-BUILDINGNUMBER': 60, 'I-BUILDINGNUMBER': 61, 'B-VEHICLEVRM': 62, 'I-VEHICLEVRM': 63, 'B-PHONEIMEI': 64, 'I-PHONEIMEI': 65, 'I-JOBAREA': 66, 'I-STATE': 67, 'B-COUNTY': 68, 'B-CURRENCYNAME': 69, 'I-CURRENCYNAME': 70, 'B-CURRENCYSYMBOL': 71, 'B-MASKEDNUMBER': 72, 'I-MASKEDNUMBER': 73, 'B-PHONE_NUMBER': 74, 'I-PHONE_NUMBER': 75, 'B-SECONDARYADDRESS': 76, 'I-SECONDARYADDRESS': 77, 'B-SSN': 78, 'I-SSN': 79, 'B-CURRENCYCODE': 80, 'B-LITECOINADDRESS': 81, 'I-LITECOINADDRESS': 82, 'B-MAC': 83, 'I-MAC': 84, 'B-CREDITCARDISSUER': 85, 'I-CREDITCARDISSUER': 86, 'B-CREDITCARDCVV': 87, 'I-CREDITCARDCVV': 88, 'B-USERAGENT': 89, 'I-USERAGENT': 90, 'B-IP': 91, 'I-IP': 92, 'B-SEX': 93, 'B-STREET': 94, 'I-STREET': 95, 'B-PIN': 96, 'I-PIN': 97, 'I-JOBTYPE': 98, 'I-MIDDLENAME': 99, 'I-CURRENCYCODE': 100, 'I-CURRENCYSYMBOL': 101, 'B-FULLNAME': 102, 'I-FULLNAME': 103, 'B-NAME': 104, 'I-NAME': 105, 'B-GENDER': 106, 'B-NUMBER': 107, 'I-NUMBER': 108, 'I-GENDER': 109, 'B-NEARBYGPSCOORDINATE': 110, 'I-NEARBYGPSCOORDINATE': 111, 'B-DISPLAYNAME': 112, 'I-DISPLAYNAME': 113, 'B-SEXTYPE': 114, 'B-ORDINALDIRECTION': 115}\n",
            "Model's ID to Label: {0: 'B-PREFIX', 1: 'I-PREFIX', 2: 'B-FIRSTNAME', 3: 'I-FIRSTNAME', 4: 'B-MIDDLENAME', 5: 'B-LASTNAME', 6: 'I-LASTNAME', 7: 'O', 8: 'B-JOBDESCRIPTOR', 9: 'B-JOBTITLE', 10: 'I-JOBTITLE', 11: 'B-COMPANY_NAME', 12: 'I-COMPANY_NAME', 13: 'B-JOBAREA', 14: 'B-EMAIL', 15: 'I-EMAIL', 16: 'B-TIME', 17: 'I-TIME', 18: 'B-DATE', 19: 'I-DATE', 20: 'B-URL', 21: 'I-URL', 22: 'B-BITCOINADDRESS', 23: 'I-BITCOINADDRESS', 24: 'B-ETHEREUMADDRESS', 25: 'I-ETHEREUMADDRESS', 26: 'B-ACCOUNTNAME', 27: 'I-ACCOUNTNAME', 28: 'B-IBAN', 29: 'I-IBAN', 30: 'B-ACCOUNTNUMBER', 31: 'I-ACCOUNTNUMBER', 32: 'B-BIC', 33: 'I-BIC', 34: 'B-IPV4', 35: 'I-IPV4', 36: 'B-STREETADDRESS', 37: 'I-STREETADDRESS', 38: 'B-CITY', 39: 'I-CITY', 40: 'B-ZIPCODE', 41: 'I-ZIPCODE', 42: 'B-USERNAME', 43: 'I-USERNAME', 44: 'B-IPV6', 45: 'I-IPV6', 46: 'B-CREDITCARDNUMBER', 47: 'I-CREDITCARDNUMBER', 48: 'B-VEHICLEVIN', 49: 'I-VEHICLEVIN', 50: 'B-SUFFIX', 51: 'I-SUFFIX', 52: 'B-AMOUNT', 53: 'I-AMOUNT', 54: 'B-CURRENCY', 55: 'I-CURRENCY', 56: 'B-PASSWORD', 57: 'I-PASSWORD', 58: 'B-JOBTYPE', 59: 'B-STATE', 60: 'B-BUILDINGNUMBER', 61: 'I-BUILDINGNUMBER', 62: 'B-VEHICLEVRM', 63: 'I-VEHICLEVRM', 64: 'B-PHONEIMEI', 65: 'I-PHONEIMEI', 66: 'I-JOBAREA', 67: 'I-STATE', 68: 'B-COUNTY', 69: 'B-CURRENCYNAME', 70: 'I-CURRENCYNAME', 71: 'B-CURRENCYSYMBOL', 72: 'B-MASKEDNUMBER', 73: 'I-MASKEDNUMBER', 74: 'B-PHONE_NUMBER', 75: 'I-PHONE_NUMBER', 76: 'B-SECONDARYADDRESS', 77: 'I-SECONDARYADDRESS', 78: 'B-SSN', 79: 'I-SSN', 80: 'B-CURRENCYCODE', 81: 'B-LITECOINADDRESS', 82: 'I-LITECOINADDRESS', 83: 'B-MAC', 84: 'I-MAC', 85: 'B-CREDITCARDISSUER', 86: 'I-CREDITCARDISSUER', 87: 'B-CREDITCARDCVV', 88: 'I-CREDITCARDCVV', 89: 'B-USERAGENT', 90: 'I-USERAGENT', 91: 'B-IP', 92: 'I-IP', 93: 'B-SEX', 94: 'B-STREET', 95: 'I-STREET', 96: 'B-PIN', 97: 'I-PIN', 98: 'I-JOBTYPE', 99: 'I-MIDDLENAME', 100: 'I-CURRENCYCODE', 101: 'I-CURRENCYSYMBOL', 102: 'B-FULLNAME', 103: 'I-FULLNAME', 104: 'B-NAME', 105: 'I-NAME', 106: 'B-GENDER', 107: 'B-NUMBER', 108: 'I-NUMBER', 109: 'I-GENDER', 110: 'B-NEARBYGPSCOORDINATE', 111: 'I-NEARBYGPSCOORDINATE', 112: 'B-DISPLAYNAME', 113: 'I-DISPLAYNAME', 114: 'B-SEXTYPE', 115: 'B-ORDINALDIRECTION'}\n",
            "116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metrics calculation function\n",
        "def compute_metrics(predictions, references):\n",
        "    acc = accuracy_score(references, predictions)\n",
        "    precision = precision_score(references, predictions, average=\"weighted\", zero_division=1)\n",
        "    recall = recall_score(references, predictions, average=\"weighted\", zero_division=1)\n",
        "    f1 = f1_score(references, predictions, average=\"weighted\", zero_division=1)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "jIZgLzW5Blk6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "def tokenize_and_align_labels(df, tokenizer, data_label_to_id, model_label_to_id):\n",
        "    def normalize_label(label):\n",
        "        # Remove suffixes like \"_1\", \"_2\"\n",
        "        if \"_\" in label and label.split(\"_\")[-1].isdigit():\n",
        "            return \"_\".join(label.split(\"_\")[:-1])\n",
        "        return label\n",
        "\n",
        "    def map_labels_to_model(data_label):\n",
        "        normalized_label = normalize_label(data_label)  # Normalize dataset labels\n",
        "        return model_label_to_id.get(normalized_label, model_label_to_id.get(\"O\", -100))  # Default to \"O\"\n",
        "\n",
        "    # Tokenize input text\n",
        "    tokenized_inputs = tokenizer(\n",
        "        list(df[\"unmasked_text\"]),  # Convert Series to list of strings\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\",\n",
        "        is_split_into_words=False,\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label_seq in enumerate(df[\"bio_labels\"]):\n",
        "        word_labels = eval(label_seq) if isinstance(label_seq, str) else label_seq  # Convert string back to list if needed\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens back to words\n",
        "        token_labels = []\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:  # Special tokens\n",
        "                token_labels.append(-100)\n",
        "            elif word_id < len(word_labels):  # Ensure within bounds\n",
        "                token_labels.append(map_labels_to_model(word_labels[word_id]))\n",
        "            else:\n",
        "                token_labels.append(-100)  # Ignore out-of-bounds tokens\n",
        "        labels.append(token_labels)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Tokenize the training set\n",
        "tokenized_train = tokenize_and_align_labels(eng_train, tokenizer, data_label_to_id, model_label_to_id)\n",
        "\n",
        "# Create a DataLoader for the test set\n",
        "train_dataset = torch.utils.data.TensorDataset(tokenized_train[\"input_ids\"], tokenized_train[\"attention_mask\"], tokenized_train[\"labels\"])\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Tokenize the test set\n",
        "tokenized_test = tokenize_and_align_labels(eng_test, tokenizer, data_label_to_id, model_label_to_id)\n",
        "\n",
        "# Create a DataLoader for the test set\n",
        "test_dataset = torch.utils.data.TensorDataset(tokenized_test[\"input_ids\"], tokenized_test[\"attention_mask\"], tokenized_test[\"labels\"])\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "jZZQbVIGVT4t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in tokenized_train.items():\n",
        "    print(f\"{key}: {value.shape}\")\n",
        "\n",
        "for key, value in tokenized_test.items():\n",
        "    print(f\"{key}: {value.shape}\")"
      ],
      "metadata": {
        "id": "TxQy4aBn4R4x",
        "outputId": "ad4a80c3-960e-49d8-b79f-0e414c72a0de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids: torch.Size([267, 140])\n",
            "token_type_ids: torch.Size([267, 140])\n",
            "attention_mask: torch.Size([267, 140])\n",
            "labels: torch.Size([267, 140])\n",
            "input_ids: torch.Size([68, 105])\n",
            "token_type_ids: torch.Size([68, 105])\n",
            "attention_mask: torch.Size([68, 105])\n",
            "labels: torch.Size([68, 105])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_label = {v: k for k, v in model_label_to_id.items()}"
      ],
      "metadata": {
        "id": "sP2XUA8E26qB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Find the optimized hyperparameters\n",
        "\n",
        "# import optuna\n",
        "\n",
        "# def objective(trial):\n",
        "#     # Suggest hyperparameters\n",
        "#     batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "#     temperature = trial.suggest_float(\"temperature\", 0.5, 2.0)\n",
        "#     use_fp16 = trial.suggest_categorical(\"use_fp16\", [True, False])\n",
        "\n",
        "#     # Configure inference settings\n",
        "#     all_predictions, all_references = [], []\n",
        "#     model.eval()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch in test_dataloader:\n",
        "#             input_ids, attention_mask, labels = [t.to(device) for t in batch]\n",
        "\n",
        "#             # Mixed precision inference\n",
        "#             with torch.cuda.amp.autocast(enabled=use_fp16):\n",
        "#                 outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#                 logits = outputs.logits / temperature  # Apply temperature scaling\n",
        "\n",
        "#             # Get predictions\n",
        "#             predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "#             labels = labels.cpu().numpy()\n",
        "\n",
        "#             # Align predictions and references to only include non-special tokens\n",
        "#             for pred, label in zip(predictions, labels):\n",
        "#                 true_labels = [id_to_label[l] for l in label if l != -100]\n",
        "#                 pred_labels = [\n",
        "#                     id_to_label.get(p, \"O\") for p, l in zip(pred, label) if l != -100\n",
        "#                 ]\n",
        "#                 all_predictions.extend(pred_labels)\n",
        "#                 all_references.extend(true_labels)\n",
        "\n",
        "#     # Calculate metrics\n",
        "#     metrics = compute_metrics(all_predictions, all_references)\n",
        "#     return metrics[\"f1\"]  # Optimize for F1 score\n",
        "\n",
        "# # Hyperparameter optimization with Optuna\n",
        "# study = optuna.create_study(direction=\"maximize\")\n",
        "# study.optimize(objective, n_trials=20)\n",
        "\n",
        "# # Display the best hyperparameters\n",
        "# print(\"Best hyperparameters:\", study.best_params)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "S558M1j-TqN8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Initialize AMP scaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Calculate the total number of training steps\n",
        "num_epochs = 5\n",
        "num_training_steps = len(train_dataloader) * num_epochs\n",
        "num_warmup_steps = int(0.1 * num_training_steps)  # Use 10% of steps for warmup\n",
        "\n",
        "# Define optimizer\n",
        "learning_rate = 2e-5  # Start with a small learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Initialize the learning rate scheduler (optional)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "total_loss = 0\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # Loop over batches in train_dataloader\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attention_mask, labels = [t.to(device) for t in batch]\n",
        "\n",
        "        # Enable mixed precision for memory efficiency\n",
        "        with autocast():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update the learning rate scheduler\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Log progress every 10 steps\n",
        "        if step % 10 == 0:  # Log every 10 steps\n",
        "            avg_loss = total_loss / (step + 1)\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Step {step}/{num_training_steps}, Loss: {loss.item()}, Avg Loss: {avg_loss}\")\n",
        "\n",
        "        step += 1\n",
        "        if step >= num_training_steps:  # Stop if maximum steps reached\n",
        "            break\n",
        "    if step >= num_training_steps:\n",
        "        break"
      ],
      "metadata": {
        "id": "BbugsyzYtt9N",
        "outputId": "00ef6ee9-ffdc-4bdd-ca43-4cf352e0b65f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-19200e941cba>:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "<ipython-input-17-19200e941cba>:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Step 0/45, Loss: 4.757996559143066, Avg Loss: 4.757996559143066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference loop using the optimized hyperparameters\n",
        "all_predictions, all_references = [], []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, labels = [t.to(device) for t in batch]\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits / 0.9989592666538125  # Use optimized temperature\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "        # Align predictions and references to only include non-special tokens\n",
        "        for pred, label in zip(predictions, labels):\n",
        "            true_labels = [\n",
        "                id_to_label[l] for l in label if l != -100\n",
        "            ]  # Convert ground truth IDs to labels\n",
        "            pred_labels = [\n",
        "                id_to_label.get(p, \"O\") for p, l in zip(pred, label) if l != -100\n",
        "            ]  # Convert predictions to labels, defaulting to \"O\" for unmapped IDs\n",
        "\n",
        "            all_predictions.extend(pred_labels)\n",
        "            all_references.extend(true_labels)\n",
        "\n",
        "# Calculate metrics\n",
        "metrics = compute_metrics(all_predictions, all_references)\n",
        "print(\"Metrics:\", metrics)"
      ],
      "metadata": {
        "id": "Om85vp-Suh8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"all predictions: {all_predictions}\")\n",
        "print(f\"number of all predictions: {len(all_predictions)}\")\n",
        "print(f\"all references: {all_references}\")\n",
        "print(f\"number of all references: {len(all_references)}\")"
      ],
      "metadata": {
        "id": "UxNW0Of2OPHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Extract the unique classes from all_references and all_predictions\n",
        "unique_labels = sorted(set(all_references) | set(all_predictions))\n",
        "display_labels = [id_to_label[label] for label in unique_labels]\n",
        "\n",
        "# Plot the confusion matrix with consistent labels\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    all_references,\n",
        "    all_predictions,\n",
        "    display_labels=display_labels\n",
        ")\n",
        "plt.title(\"Confusion Matrix (Filtered Labels)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HUL9-vXNULkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "report = classification_report(all_references, all_predictions, output_dict=True)\n",
        "df = pd.DataFrame(report).transpose()\n",
        "\n",
        "# Plot F1 scores for each label\n",
        "df[\"f1-score\"].plot(kind=\"bar\", figsize=(12, 6))\n",
        "plt.title(\"F1 Score by Label\")\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.xlabel(\"Labels\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kl10gPyPUEii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "cm = confusion_matrix(all_references, all_predictions)\n",
        "misclassified_indices = np.unravel_index(np.argsort(-cm, axis=None), cm.shape)\n",
        "top_misclassifications = [\n",
        "    (id_to_label[misclassified_indices[0][i]], id_to_label[misclassified_indices[1][i]], cm[misclassified_indices[0][i], misclassified_indices[1][i]])\n",
        "    for i in range(10)  # Top 10 misclassifications\n",
        "]\n",
        "\n",
        "for true, predicted, count in top_misclassifications:\n",
        "    print(f\"True: {true}, Predicted: {predicted}, Count: {count}\")\n",
        "\n",
        "# Extract unique indices for the top misclassifications\n",
        "unique_indices = list(set([misclassified_indices[0][i] for i in range(10)] +\n",
        "                          [misclassified_indices[1][i] for i in range(10)]))\n",
        "\n",
        "# Create a subset confusion matrix\n",
        "subset_cm = cm[np.ix_(unique_indices, unique_indices)]\n",
        "subset_labels = [list(id_to_label.values())[i] for i in unique_indices]\n",
        "\n",
        "# Plot the subset confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=subset_cm, display_labels=subset_labels)\n",
        "disp.plot(cmap=\"viridis\", xticks_rotation=\"vertical\", colorbar=True)\n",
        "plt.title(\"Confusion Matrix For Top Misclassifications)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RtLhExglP5IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "# Convert string labels to numeric IDs using id_to_label reverse mapping\n",
        "label_to_id_reverse = {label: idx for idx, label in id_to_label.items()}\n",
        "\n",
        "# Map predictions and references to numeric IDs\n",
        "numeric_predictions = [label_to_id_reverse[label] for label in all_predictions]\n",
        "numeric_references = [label_to_id_reverse[label] for label in all_references]\n",
        "\n",
        "# Choose the label ID of interest (e.g., for binary classification, the positive class)\n",
        "pos_label_id = label_to_id_reverse[\"B-CREDITCARDNUMBER\"]  # Replace \"B-YOUR_LABEL\" with your target class\n",
        "\n",
        "# Compute precision-recall curve\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(numeric_references, numeric_predictions, pos_label=pos_label_id)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n",
        "\n",
        "# Flatten predictions and references for multiclass handling (if applicable)\n",
        "# Note: This works best for binary or one-vs-all analysis.\n",
        "# Example below assumes binary or flattened labels.\n",
        "precision, recall, _ = precision_recall_curve(numeric_references, numeric_predictions, pos_label=\"B-CREDITCARDNUMBER\")\n",
        "\n",
        "# Display Precision-Recall curve\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yiDIkqd3Mgv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
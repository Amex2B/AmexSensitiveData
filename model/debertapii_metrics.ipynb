{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "Fmn9974mQ_be",
        "outputId": "735d01f5-17a4-4161-966d-846264cd29b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chardet"
      ],
      "metadata": {
        "id": "NOGaH53HiueD",
        "outputId": "05db4416-d42d-4f3f-dcca-9f3023f97c76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "lgd9lg-0OqVX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and pre-trained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lakshyakh93/deberta_finetuned_pii\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"lakshyakh93/deberta_finetuned_pii\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "9Zd8IjNWjCLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "import chardet\n",
        "\n",
        "filepath = os.path.join(os.getcwd(), 'mini_df_200_row_biolabels.csv')\n",
        "\n",
        "# Detect encoding\n",
        "with open(filepath, 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "    print(\"Detected file encoding:\", result['encoding'])\n",
        "\n",
        "# Read the file using the detected encoding\n",
        "df = pd.read_csv(filepath, encoding=result['encoding'])\n",
        "\n",
        "# Proportional split ratios\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Since you have 200 rows, no need for the 3,000 cap, as this won't be needed\n",
        "train_size = int(len(df) * train_ratio)\n",
        "val_size = int(len(df) * val_ratio)\n",
        "test_size = len(df) - train_size - val_size  # Remaining rows for the test set\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_df, temp_df = train_test_split(df, test_size=1 - train_ratio, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=test_ratio / (test_ratio + val_ratio), random_state=42)\n",
        "\n",
        "# Print dataset sizes to confirm\n",
        "print(f\"Training size: {len(train_df)}\")\n",
        "print(f\"Validation size: {len(val_df)}\")\n",
        "print(f\"Test size: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "R9NmJEwO-LUr",
        "outputId": "be3e35cc-bfc6-416a-9dd4-98dff810e5ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected file encoding: Windows-1252\n",
            "Training size: 134\n",
            "Validation size: 29\n",
            "Test size: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping for the bio_labels\n",
        "data_label_list = []\n",
        "\n",
        "# Extract unique labels from the 'bio_labels' column\n",
        "def get_unique_labels(df):\n",
        "    unique_labels = set()\n",
        "    for labels in df['bio_labels']:  # Iterate through the 'bio_labels' column\n",
        "        # Since bio_labels is a string representation of a list, we need to convert it back to a list\n",
        "        labels = eval(labels)  # Convert the string to a list\n",
        "        unique_labels.update(labels)  # Add all labels in the current row to the set\n",
        "    return sorted(unique_labels)  # Return sorted labels for consistency\n",
        "\n",
        "# Apply the function to the entire DataFrame\n",
        "unique_labels = get_unique_labels(df)  # Run on the whole DataFrame\n",
        "\n",
        "# Print the unique labels\n",
        "print(\"Data's Unique BIO Labels:\", unique_labels)\n",
        "\n",
        "# Combine with the existing label_list (if applicable)\n",
        "data_label_list = sorted(set(data_label_list + unique_labels))  # Ensure no duplicates\n",
        "print(\"Data's Final Label List:\", data_label_list)\n",
        "print(len(data_label_list))  # Number of unique labels\n",
        "\n",
        "data_label_to_id = {label: idx for idx, label in enumerate(data_label_list)}\n",
        "data_id_to_label = {idx: label for label, idx in data_label_to_id.items()}"
      ],
      "metadata": {
        "id": "BIOvLTGTFKfZ",
        "outputId": "18730f82-ec6e-415d-efbb-4eecf49a0993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data's Unique BIO Labels: ['B-ACCOUNTNAME_1', 'B-ACCOUNTNUMBER_1', 'B-AGE_1', 'B-AMOUNT_1', 'B-BIC_1', 'B-BITCOINADDRESS_1', 'B-BUILDINGNUMBER_1', 'B-CITY_1', 'B-COMPANYNAME_1', 'B-COUNTY_1', 'B-CREDITCARDCVV_1', 'B-CREDITCARDISSUER_1', 'B-CREDITCARDNUMBER_1', 'B-CURRENCYNAME_1', 'B-CURRENCYSYMBOL_1', 'B-CURRENCY_1', 'B-DATE_1', 'B-DOB_1', 'B-EMAIL_1', 'B-ETHEREUMADDRESS_1', 'B-EYECOLOR_1', 'B-FIRSTNAME_1', 'B-FIRSTNAME_2', 'B-GENDER_1', 'B-HEIGHT_1', 'B-IBAN_1', 'B-IPV4_1', 'B-IPV6_1', 'B-JOBAREA_1', 'B-JOBTITLE_1', 'B-JOBTYPE_1', 'B-LASTNAME_1', 'B-LITECOINADDRESS_1', 'B-MAC_1', 'B-MASKEDNUMBER_1', 'B-MEDICAL_1', 'B-MIDDLENAME_1', 'B-NEARBYGPSCOORDINATE_1', 'B-ORDINALDIRECTION_1', 'B-ORGANIZATION_1', 'B-PASSWORD_1', 'B-PHONEIMEI_1', 'B-PHONENUMBER_1', 'B-PIN_1', 'B-PREFIX_1', 'B-SECONDARYADDRESS_1', 'B-SEX_1', 'B-SSN_1', 'B-STATE_1', 'B-STREET_1', 'B-TIME_1', 'B-URL_1', 'B-USERNAME_1', 'B-VEHICLEVIN_1', 'B-VEHICLEVRM_1', 'B-ZIPCODE_1', 'I-ACCOUNTNAME_1', 'I-AGE_1', 'I-BUILDINGNUMBER_1', 'I-CITY_1', 'I-COMPANYNAME_1', 'I-COUNTY_1', 'I-CURRENCYNAME_1', 'I-CURRENCYSYMBOL_1', 'I-CURRENCY_1', 'I-DATE_1', 'I-DOB_1', 'I-EYECOLOR_1', 'I-GENDER_1', 'I-JOBAREA_1', 'I-JOBTITLE_1', 'I-LASTNAME_1', 'I-MEDICAL_1', 'I-ORGANIZATION_1', 'I-PHONENUMBER_1', 'I-SECONDARYADDRESS_1', 'I-SEX_1', 'I-SSN_1', 'I-STATE_1', 'I-STREET_1', 'I-TIME_1', 'O']\n",
            "Data's Final Label List: ['B-ACCOUNTNAME_1', 'B-ACCOUNTNUMBER_1', 'B-AGE_1', 'B-AMOUNT_1', 'B-BIC_1', 'B-BITCOINADDRESS_1', 'B-BUILDINGNUMBER_1', 'B-CITY_1', 'B-COMPANYNAME_1', 'B-COUNTY_1', 'B-CREDITCARDCVV_1', 'B-CREDITCARDISSUER_1', 'B-CREDITCARDNUMBER_1', 'B-CURRENCYNAME_1', 'B-CURRENCYSYMBOL_1', 'B-CURRENCY_1', 'B-DATE_1', 'B-DOB_1', 'B-EMAIL_1', 'B-ETHEREUMADDRESS_1', 'B-EYECOLOR_1', 'B-FIRSTNAME_1', 'B-FIRSTNAME_2', 'B-GENDER_1', 'B-HEIGHT_1', 'B-IBAN_1', 'B-IPV4_1', 'B-IPV6_1', 'B-JOBAREA_1', 'B-JOBTITLE_1', 'B-JOBTYPE_1', 'B-LASTNAME_1', 'B-LITECOINADDRESS_1', 'B-MAC_1', 'B-MASKEDNUMBER_1', 'B-MEDICAL_1', 'B-MIDDLENAME_1', 'B-NEARBYGPSCOORDINATE_1', 'B-ORDINALDIRECTION_1', 'B-ORGANIZATION_1', 'B-PASSWORD_1', 'B-PHONEIMEI_1', 'B-PHONENUMBER_1', 'B-PIN_1', 'B-PREFIX_1', 'B-SECONDARYADDRESS_1', 'B-SEX_1', 'B-SSN_1', 'B-STATE_1', 'B-STREET_1', 'B-TIME_1', 'B-URL_1', 'B-USERNAME_1', 'B-VEHICLEVIN_1', 'B-VEHICLEVRM_1', 'B-ZIPCODE_1', 'I-ACCOUNTNAME_1', 'I-AGE_1', 'I-BUILDINGNUMBER_1', 'I-CITY_1', 'I-COMPANYNAME_1', 'I-COUNTY_1', 'I-CURRENCYNAME_1', 'I-CURRENCYSYMBOL_1', 'I-CURRENCY_1', 'I-DATE_1', 'I-DOB_1', 'I-EYECOLOR_1', 'I-GENDER_1', 'I-JOBAREA_1', 'I-JOBTITLE_1', 'I-LASTNAME_1', 'I-MEDICAL_1', 'I-ORGANIZATION_1', 'I-PHONENUMBER_1', 'I-SECONDARYADDRESS_1', 'I-SEX_1', 'I-SSN_1', 'I-STATE_1', 'I-STREET_1', 'I-TIME_1', 'O']\n",
            "82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_label_list = model.config.id2label.values()  # Ensure this matches the fine-tuned model\n",
        "model_id_to_label = {int(k): v for k, v in model.config.id2label.items()}\n",
        "model_label_to_id = {v: int(k) for k, v in model_id_to_label.items()}\n",
        "\n",
        "print(\"Model's Label to ID:\", model_label_to_id)\n",
        "print(\"Model's ID to Label:\", model_id_to_label)\n",
        "print(len(model_label_list))"
      ],
      "metadata": {
        "id": "EejcPQbmwrK-",
        "outputId": "c1f4c253-a6f9-4f3f-f683-7b63f03a9a29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's Label to ID: {'B-PREFIX': 0, 'I-PREFIX': 1, 'B-FIRSTNAME': 2, 'I-FIRSTNAME': 3, 'B-MIDDLENAME': 4, 'B-LASTNAME': 5, 'I-LASTNAME': 6, 'O': 7, 'B-JOBDESCRIPTOR': 8, 'B-JOBTITLE': 9, 'I-JOBTITLE': 10, 'B-COMPANY_NAME': 11, 'I-COMPANY_NAME': 12, 'B-JOBAREA': 13, 'B-EMAIL': 14, 'I-EMAIL': 15, 'B-TIME': 16, 'I-TIME': 17, 'B-DATE': 18, 'I-DATE': 19, 'B-URL': 20, 'I-URL': 21, 'B-BITCOINADDRESS': 22, 'I-BITCOINADDRESS': 23, 'B-ETHEREUMADDRESS': 24, 'I-ETHEREUMADDRESS': 25, 'B-ACCOUNTNAME': 26, 'I-ACCOUNTNAME': 27, 'B-IBAN': 28, 'I-IBAN': 29, 'B-ACCOUNTNUMBER': 30, 'I-ACCOUNTNUMBER': 31, 'B-BIC': 32, 'I-BIC': 33, 'B-IPV4': 34, 'I-IPV4': 35, 'B-STREETADDRESS': 36, 'I-STREETADDRESS': 37, 'B-CITY': 38, 'I-CITY': 39, 'B-ZIPCODE': 40, 'I-ZIPCODE': 41, 'B-USERNAME': 42, 'I-USERNAME': 43, 'B-IPV6': 44, 'I-IPV6': 45, 'B-CREDITCARDNUMBER': 46, 'I-CREDITCARDNUMBER': 47, 'B-VEHICLEVIN': 48, 'I-VEHICLEVIN': 49, 'B-SUFFIX': 50, 'I-SUFFIX': 51, 'B-AMOUNT': 52, 'I-AMOUNT': 53, 'B-CURRENCY': 54, 'I-CURRENCY': 55, 'B-PASSWORD': 56, 'I-PASSWORD': 57, 'B-JOBTYPE': 58, 'B-STATE': 59, 'B-BUILDINGNUMBER': 60, 'I-BUILDINGNUMBER': 61, 'B-VEHICLEVRM': 62, 'I-VEHICLEVRM': 63, 'B-PHONEIMEI': 64, 'I-PHONEIMEI': 65, 'I-JOBAREA': 66, 'I-STATE': 67, 'B-COUNTY': 68, 'B-CURRENCYNAME': 69, 'I-CURRENCYNAME': 70, 'B-CURRENCYSYMBOL': 71, 'B-MASKEDNUMBER': 72, 'I-MASKEDNUMBER': 73, 'B-PHONE_NUMBER': 74, 'I-PHONE_NUMBER': 75, 'B-SECONDARYADDRESS': 76, 'I-SECONDARYADDRESS': 77, 'B-SSN': 78, 'I-SSN': 79, 'B-CURRENCYCODE': 80, 'B-LITECOINADDRESS': 81, 'I-LITECOINADDRESS': 82, 'B-MAC': 83, 'I-MAC': 84, 'B-CREDITCARDISSUER': 85, 'I-CREDITCARDISSUER': 86, 'B-CREDITCARDCVV': 87, 'I-CREDITCARDCVV': 88, 'B-USERAGENT': 89, 'I-USERAGENT': 90, 'B-IP': 91, 'I-IP': 92, 'B-SEX': 93, 'B-STREET': 94, 'I-STREET': 95, 'B-PIN': 96, 'I-PIN': 97, 'I-JOBTYPE': 98, 'I-MIDDLENAME': 99, 'I-CURRENCYCODE': 100, 'I-CURRENCYSYMBOL': 101, 'B-FULLNAME': 102, 'I-FULLNAME': 103, 'B-NAME': 104, 'I-NAME': 105, 'B-GENDER': 106, 'B-NUMBER': 107, 'I-NUMBER': 108, 'I-GENDER': 109, 'B-NEARBYGPSCOORDINATE': 110, 'I-NEARBYGPSCOORDINATE': 111, 'B-DISPLAYNAME': 112, 'I-DISPLAYNAME': 113, 'B-SEXTYPE': 114, 'B-ORDINALDIRECTION': 115}\n",
            "Model's ID to Label: {0: 'B-PREFIX', 1: 'I-PREFIX', 2: 'B-FIRSTNAME', 3: 'I-FIRSTNAME', 4: 'B-MIDDLENAME', 5: 'B-LASTNAME', 6: 'I-LASTNAME', 7: 'O', 8: 'B-JOBDESCRIPTOR', 9: 'B-JOBTITLE', 10: 'I-JOBTITLE', 11: 'B-COMPANY_NAME', 12: 'I-COMPANY_NAME', 13: 'B-JOBAREA', 14: 'B-EMAIL', 15: 'I-EMAIL', 16: 'B-TIME', 17: 'I-TIME', 18: 'B-DATE', 19: 'I-DATE', 20: 'B-URL', 21: 'I-URL', 22: 'B-BITCOINADDRESS', 23: 'I-BITCOINADDRESS', 24: 'B-ETHEREUMADDRESS', 25: 'I-ETHEREUMADDRESS', 26: 'B-ACCOUNTNAME', 27: 'I-ACCOUNTNAME', 28: 'B-IBAN', 29: 'I-IBAN', 30: 'B-ACCOUNTNUMBER', 31: 'I-ACCOUNTNUMBER', 32: 'B-BIC', 33: 'I-BIC', 34: 'B-IPV4', 35: 'I-IPV4', 36: 'B-STREETADDRESS', 37: 'I-STREETADDRESS', 38: 'B-CITY', 39: 'I-CITY', 40: 'B-ZIPCODE', 41: 'I-ZIPCODE', 42: 'B-USERNAME', 43: 'I-USERNAME', 44: 'B-IPV6', 45: 'I-IPV6', 46: 'B-CREDITCARDNUMBER', 47: 'I-CREDITCARDNUMBER', 48: 'B-VEHICLEVIN', 49: 'I-VEHICLEVIN', 50: 'B-SUFFIX', 51: 'I-SUFFIX', 52: 'B-AMOUNT', 53: 'I-AMOUNT', 54: 'B-CURRENCY', 55: 'I-CURRENCY', 56: 'B-PASSWORD', 57: 'I-PASSWORD', 58: 'B-JOBTYPE', 59: 'B-STATE', 60: 'B-BUILDINGNUMBER', 61: 'I-BUILDINGNUMBER', 62: 'B-VEHICLEVRM', 63: 'I-VEHICLEVRM', 64: 'B-PHONEIMEI', 65: 'I-PHONEIMEI', 66: 'I-JOBAREA', 67: 'I-STATE', 68: 'B-COUNTY', 69: 'B-CURRENCYNAME', 70: 'I-CURRENCYNAME', 71: 'B-CURRENCYSYMBOL', 72: 'B-MASKEDNUMBER', 73: 'I-MASKEDNUMBER', 74: 'B-PHONE_NUMBER', 75: 'I-PHONE_NUMBER', 76: 'B-SECONDARYADDRESS', 77: 'I-SECONDARYADDRESS', 78: 'B-SSN', 79: 'I-SSN', 80: 'B-CURRENCYCODE', 81: 'B-LITECOINADDRESS', 82: 'I-LITECOINADDRESS', 83: 'B-MAC', 84: 'I-MAC', 85: 'B-CREDITCARDISSUER', 86: 'I-CREDITCARDISSUER', 87: 'B-CREDITCARDCVV', 88: 'I-CREDITCARDCVV', 89: 'B-USERAGENT', 90: 'I-USERAGENT', 91: 'B-IP', 92: 'I-IP', 93: 'B-SEX', 94: 'B-STREET', 95: 'I-STREET', 96: 'B-PIN', 97: 'I-PIN', 98: 'I-JOBTYPE', 99: 'I-MIDDLENAME', 100: 'I-CURRENCYCODE', 101: 'I-CURRENCYSYMBOL', 102: 'B-FULLNAME', 103: 'I-FULLNAME', 104: 'B-NAME', 105: 'I-NAME', 106: 'B-GENDER', 107: 'B-NUMBER', 108: 'I-NUMBER', 109: 'I-GENDER', 110: 'B-NEARBYGPSCOORDINATE', 111: 'I-NEARBYGPSCOORDINATE', 112: 'B-DISPLAYNAME', 113: 'I-DISPLAYNAME', 114: 'B-SEXTYPE', 115: 'B-ORDINALDIRECTION'}\n",
            "116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metrics calculation function\n",
        "def compute_metrics(predictions, references):\n",
        "    acc = accuracy_score(references, predictions)\n",
        "    precision = precision_score(references, predictions, average=\"weighted\", zero_division=1)\n",
        "    recall = recall_score(references, predictions, average=\"weighted\", zero_division=1)\n",
        "    f1 = f1_score(references, predictions, average=\"weighted\", zero_division=1)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "jIZgLzW5Blk6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run inference and calculate the metrics on the 200-row dataset\n",
        "don't need to train it"
      ],
      "metadata": {
        "id": "c2VdQajaOWZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Prepare data for tokenization and create a PyTorch Dataset\n",
        "def tokenize_and_align_labels(df, tokenizer, data_label_to_id, model_label_to_id):\n",
        "    def normalize_label(label):\n",
        "        # Remove suffixes like \"_1\", \"_2\"\n",
        "        if \"_\" in label and label.split(\"_\")[-1].isdigit():\n",
        "            return \"_\".join(label.split(\"_\")[:-1])\n",
        "        return label\n",
        "\n",
        "    def map_labels_to_model(data_label):\n",
        "        normalized_label = normalize_label(data_label)  # Normalize dataset labels\n",
        "        return model_label_to_id.get(normalized_label, model_label_to_id.get(\"O\", -100))  # Default to \"O\"\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        list(df[\"unmasked_text\"]),  # Convert Series to list of strings\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\",\n",
        "        is_split_into_words=False,\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label_seq in enumerate(df[\"bio_labels\"]):\n",
        "        word_labels = eval(label_seq)  # Convert string back to Python list\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens back to words\n",
        "        token_labels = []\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:  # Special tokens\n",
        "                token_labels.append(-100)\n",
        "            elif word_id < len(word_labels):  # Ensure within bounds\n",
        "                token_labels.append(map_labels_to_model(word_labels[word_id]))\n",
        "            else:\n",
        "                token_labels.append(-100)  # Ignore out-of-bounds tokens\n",
        "        labels.append(token_labels)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Tokenize the test set\n",
        "tokenized_test = tokenize_and_align_labels(df, tokenizer, data_label_to_id, model_label_to_id)\n",
        "\n",
        "# Create a DataLoader for the test set\n",
        "test_dataset = torch.utils.data.TensorDataset(tokenized_test[\"input_ids\"], tokenized_test[\"attention_mask\"], tokenized_test[\"labels\"])\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "jZZQbVIGVT4t"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in tokenized_test.items():\n",
        "    print(f\"{key}: {value.shape}\")"
      ],
      "metadata": {
        "id": "TxQy4aBn4R4x",
        "outputId": "92bf98e1-b494-450c-9483-53f5aef7e239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids: torch.Size([192, 113])\n",
            "token_type_ids: torch.Size([192, 113])\n",
            "attention_mask: torch.Size([192, 113])\n",
            "labels: torch.Size([192, 113])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_label = {v: k for k, v in model_label_to_id.items()}"
      ],
      "metadata": {
        "id": "sP2XUA8E26qB"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference loop\n",
        "all_predictions, all_references = [], []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, labels = [t.to(device) for t in batch]\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "        # Align predictions and references to only include non-special tokens\n",
        "        for pred, label in zip(predictions, labels):\n",
        "            true_labels = [\n",
        "                id_to_label[l] for l in label if l != -100\n",
        "            ]  # Convert ground truth IDs to labels\n",
        "            pred_labels = [\n",
        "                id_to_label.get(p, \"O\") for p, l in zip(pred, label) if l != -100\n",
        "            ]  # Convert predictions to labels, defaulting to \"O\" for unmapped IDs\n",
        "\n",
        "            all_predictions.extend(pred_labels)\n",
        "            all_references.extend(true_labels)\n",
        "\n",
        "# Calculate metrics\n",
        "metrics = compute_metrics(all_predictions, all_references)\n",
        "print(\"Metrics:\", metrics)"
      ],
      "metadata": {
        "id": "Om85vp-Suh8h",
        "outputId": "db8f0894-e0e0-4da7-a5f4-e3a2647b2e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics: {'accuracy': 0.6435177673211099, 'precision': 0.8197830856231896, 'recall': 0.6435177673211099, 'f1': 0.7183113567528646}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* deal with the redaction issue\n",
        "  * 'biolabels' column\n",
        "  * find the unique labels\n",
        "  * add them to the label map\n",
        "\n",
        "* don't need to finetune deBERTa anymore\n",
        "  * just calculate its accuracy\n",
        "\n",
        "* can also finetune a not-finetuned deBERTa model if you have time"
      ],
      "metadata": {
        "id": "jbYMZzHookhR"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}